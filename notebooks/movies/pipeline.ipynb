{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gz8wapLmBXyx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import ast\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import re\n",
    "import heapq\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.read_json('movies.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GenreTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.genre_map = {\n",
    "            28: 'Action',\n",
    "            12: 'Adventure',\n",
    "            16: 'Animation',\n",
    "            35: 'Comedy',\n",
    "            80: 'Crime',\n",
    "            99: 'Documentary',\n",
    "            18: 'Drama',\n",
    "            10751: 'Family',\n",
    "            14: 'Fantasy',\n",
    "            36: 'History ',\n",
    "            27: 'Horror',\n",
    "            10402: 'Music',\n",
    "            9648: 'Mystery',\n",
    "            10749: 'Romance',\n",
    "            878: 'ScienceFiction',\n",
    "            10770: 'TvMovie',\n",
    "            53: 'Thriller',\n",
    "            10752: 'War',\n",
    "            37: 'Western'\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        X_modified = X.copy(deep=True);\n",
    "        genres = list(self.genre_map.values())\n",
    "        X_modified[list(genres)] = 0\n",
    "        \n",
    "        for i in range(len(X_modified)):\n",
    "            genres = ast.literal_eval(X_modified.iloc[i]['GenreIds'])\n",
    "            for genre in genres:\n",
    "                X_modified.loc[i, self.genre_map[genre]] = 1\n",
    "        \n",
    "        # dropping the GenreIds\n",
    "        X_modified.drop('GenreIds', axis=1, inplace=True)\n",
    "        X_modified.drop('Id', axis=1, inplace=True)\n",
    "\n",
    "        # eliminating the column ProductionCompanies\n",
    "        X_modified.drop('ProductionCompanies', axis=1, inplace=True)\n",
    "\n",
    "        return X_modified\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "class LanguageTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        X_modified = X.copy(deep=True);\n",
    "        language_counts = X_modified['OriginalLanguage'].value_counts()\n",
    "        X_modified['OriginalLanguage'] = X_modified['OriginalLanguage'].map(language_counts)\n",
    "        X_modified.drop('SpokenLanguages', axis=1, inplace=True)\n",
    "        return X_modified\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "class CountryTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.country_region = {\n",
    "            'United States of America': 'North America',\n",
    "            'Italy': 'Europe',\n",
    "            'Spain': 'Europe',\n",
    "            'South Korea': 'Asia',\n",
    "            'China': 'Asia',\n",
    "            'India': 'Asia',\n",
    "            'Canada': 'North America',\n",
    "            'France': 'Europe',\n",
    "            'Australia': 'Oceania',\n",
    "            'United Kingdom': 'Europe',\n",
    "            'Germany': 'Europe',\n",
    "            'Brazil': 'South America',\n",
    "            'Mexico': 'North America',\n",
    "            'Japan': 'Asia',\n",
    "            'Russia': 'Europe',\n",
    "            'Sweden': 'Europe',\n",
    "            'United Arab Emirates': 'Asia',\n",
    "            'Nigeria': 'Africa',\n",
    "            'Poland': 'Europe',\n",
    "            'Serbia': 'Europe',\n",
    "            'Ukraine': 'Europe',\n",
    "            'Thailand': 'Asia',\n",
    "            'Finland': 'Europe',\n",
    "            'Norway': 'Europe',\n",
    "            'Switzerland': 'Europe',\n",
    "            'Bangladesh': 'Asia',\n",
    "            'Austria': 'Europe',\n",
    "            'Kazakhstan': 'Asia',\n",
    "            'Belgium': 'Europe',\n",
    "            'Hong Kong': 'Asia',\n",
    "            'Cyprus': 'Europe',\n",
    "            'Greece': 'Europe',\n",
    "            'Denmark': 'Europe',\n",
    "            'Ireland': 'Europe',\n",
    "            'New Zealand': 'Oceania',\n",
    "            'Chile': 'South America',\n",
    "            'Philippines': 'Asia',\n",
    "            'Singapore': 'Asia',\n",
    "            'Taiwan': 'Asia',\n",
    "            'Puerto Rico': 'North America',\n",
    "            'Iceland': 'Europe',\n",
    "            'Argentina': 'South America',\n",
    "            'Czech Republic': 'Europe',\n",
    "            'Colombia': 'South America',\n",
    "            'Peru': 'South America',\n",
    "            'Bulgaria': 'Europe',\n",
    "            'Netherlands': 'Europe',\n",
    "            'Hungary': 'Europe',\n",
    "            'South Africa': 'Africa',\n",
    "            'Latvia': 'Europe',\n",
    "            'Dominican Republic': 'North America',\n",
    "            'Uruguay': 'South America',\n",
    "            'Venezuela': 'South America',\n",
    "            'Malta': 'Europe',\n",
    "            'Turkey': 'Asia',\n",
    "            'Saudi Arabia': 'Asia',\n",
    "            'Portugal': 'Europe',\n",
    "            'Morocco': 'Africa',\n",
    "            'Slovenia': 'Europe',\n",
    "            'Israel': 'Asia',\n",
    "            'Luxembourg': 'Europe',\n",
    "            'Indonesia': 'Asia',\n",
    "            'Panama': 'North America',\n",
    "            'Bolivia': 'South America',\n",
    "            'Romania': 'Europe',\n",
    "            'Guadaloupe': 'North America',\n",
    "            'Iran': 'Asia',\n",
    "            'Costa Rica': 'North America',\n",
    "            'Honduras': 'North America',\n",
    "            'Albania': 'Europe',\n",
    "            'Jordan': 'Asia',\n",
    "            'Pakistan': 'Asia',\n",
    "            'Lithuania': 'Europe',\n",
    "            'Vietnam': 'Asia',\n",
    "            'Malawi': 'Africa',\n",
    "            'Soviet Union': 'Europe',\n",
    "            'Estonia': 'Europe',\n",
    "            'Botswana': 'Africa',\n",
    "            'Paraguay': 'South America',\n",
    "            'Yugoslavia': 'Europe',\n",
    "            'Georgia': 'Asia',\n",
    "            'Slovakia': 'Europe',\n",
    "            'Malaysia': 'Asia',\n",
    "            'Mauritius': 'Africa',\n",
    "            'Guatemala': 'North America',\n",
    "            'Macao': 'Asia',\n",
    "            'Jamaica': 'North America',\n",
    "            'Lebanon': 'Asia',\n",
    "            'Qatar': 'Asia',\n",
    "            'Zimbabwe': 'Africa',\n",
    "            'Egypt': 'Africa',\n",
    "            'Senegal': 'Africa',\n",
    "            'Czechoslovakia': 'Europe',\n",
    "            'East Germany': 'Europe',\n",
    "            'Kenya': 'Africa',\n",
    "            'Solomon Islands': 'Oceania',\n",
    "            'Cambodia': 'Asia',\n",
    "            'Iraq': 'Asia',\n",
    "            'Tunisia': 'Africa',\n",
    "            'Ecuador': 'South America',\n",
    "            'Croatia': 'Europe',\n",
    "            'Liechtenstein': 'Europe',\n",
    "            'Namibia': 'Africa',\n",
    "            'Ghana': 'Africa',\n",
    "            'Bahamas': 'North America',\n",
    "            'Aruba': 'North America',\n",
    "            'Moldova': 'Europe'\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        X_modified = X.copy(deep=True);\n",
    "        X_modified['Country'] = X_modified['ProductionCountries'].apply(lambda x: ast.literal_eval(x)[0]['name'] if x and ast.literal_eval(x) else None)\n",
    "        X_modified['Region'] = X_modified['Country'].map(self.country_region)\n",
    "\n",
    "        X_modified['Region'].fillna(X_modified['Region'].mode().iloc[0], inplace=True)\n",
    "\n",
    "        X_modified.drop(['ProductionCountries', 'Country'], axis=1, inplace=True)\n",
    "        X_modified.dropna(subset=['Overview'], inplace=True)\n",
    "        X_modified.dropna(subset=['ReleaseDate'], inplace=True)\n",
    "        X_modified['ReleaseDate'] = pd.to_datetime(X_modified['ReleaseDate'])\n",
    "        \n",
    "        return X_modified\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "class OtherStepsTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        X_modified = X.copy(deep=True);\n",
    "        # Feature Engineer columns from ReleaseDate\n",
    "        X_modified['ReleaseYear'] = X_modified['ReleaseDate'].dt.year\n",
    "        X_modified['ReleaseMonth'] = X_modified['ReleaseDate'].dt.month\n",
    "\n",
    "        def map_to_decade(year):\n",
    "            if np.isnan(year):  # Handling NaN values\n",
    "                return np.nan\n",
    "            return int(10 * (year // 10))\n",
    "\n",
    "        # Apply the function to create a new column 'Decade'\n",
    "        X_modified['Decade'] = X_modified['ReleaseYear'].apply(map_to_decade)\n",
    "\n",
    "        X_modified.drop('ReleaseDate', axis=1, inplace=True)\n",
    "\n",
    "        \n",
    "        return X_modified\n",
    "    \n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "class ImputeBugetTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        X_modified = X.copy(deep=True);\n",
    "        # Simple linear regression to impute the rows with 0 Budget using VoteCount\n",
    "\n",
    "        df_with_zeros = X_modified[X_modified['Budget'] == 0]\n",
    "        df_without_zeros = X_modified[X_modified['Budget'] > 0]\n",
    "\n",
    "        X_train = df_without_zeros[['VoteCount']]\n",
    "        y_train = df_without_zeros['Budget']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "        model = LinearRegression()\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        predicted_budget = model.predict(df_with_zeros[['VoteCount']])\n",
    "\n",
    "        X_modified.loc[df_with_zeros.index, 'Budget'] = predicted_budget\n",
    "\n",
    "        return X_modified\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "class PopularityTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_modified = X.copy(deep=True);\n",
    "        X_modified['Popularity_cat'] = np.digitize(X_modified['Popularity'], bins=[16, 30, 53])\n",
    "        return X_modified\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "class TrimmedTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        X_modified = X.copy(deep=True);\n",
    "        # method to return a copy of a data frame after trimming the numerical valiables\n",
    "\n",
    "        def get_trimmed_data(data, numerical_columns):\n",
    "            result = data.copy()\n",
    "            for col in numerical_columns:\n",
    "                upper_q = np.quantile(data[col], 0.75)\n",
    "                lower_q = np.quantile(data[col], 0.25)\n",
    "                IQR = upper_q - lower_q\n",
    "                upper_whisker = upper_q + (1.5 * IQR)\n",
    "                lower_whisker = lower_q - (1.5 * IQR)\n",
    "                indexes = result[(result[col] > upper_whisker) | (result[col] < lower_whisker)].index\n",
    "                result.drop(indexes, inplace=True)\n",
    "            return result\n",
    "\n",
    "        # Function to apply log tranformation to data, returns a copy with transformed columns\n",
    "        def get_log_transformed_data(data, numerical_columns):\n",
    "            result = data.copy()\n",
    "            for col in numerical_columns:\n",
    "                result[col] = result[col].map(lambda x: np.log(x) if x > 0 else 0)\n",
    "            return result\n",
    "\n",
    "        movies_data_trimmed = get_trimmed_data(X_modified, ['RunTime'])\n",
    "        X_modified = get_log_transformed_data(movies_data_trimmed, ['Budget'])\n",
    "\n",
    "        return X_modified\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "class SummarizeOverviewTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        X_modified = X.copy(deep=True);\n",
    "        # Method to summarize the overview column\n",
    "        def summarize_overview(overview_text):\n",
    "            # Check if the overview_text is a valid string\n",
    "            if isinstance(overview_text, str):\n",
    "                # Preprocessing the data\n",
    "                clean_text = re.sub(r'\\W', ' ', overview_text.lower())\n",
    "                clean_text = re.sub(r'\\d', ' ', clean_text)\n",
    "                clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "\n",
    "                # Tokenize sentences\n",
    "                sentences = nltk.sent_tokenize(clean_text)\n",
    "\n",
    "                # Stopword list\n",
    "                stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "                # Word counts\n",
    "                word2count = {}\n",
    "                for word in nltk.word_tokenize(clean_text):\n",
    "                    if word not in stop_words:\n",
    "                        if word not in word2count.keys():\n",
    "                            word2count[word] = 1\n",
    "                        else:\n",
    "                            word2count[word] += 1\n",
    "\n",
    "                # Converting counts to weights\n",
    "                max_count = max(word2count.values())\n",
    "                for key in word2count.keys():\n",
    "                    word2count[key] = word2count[key] / max_count\n",
    "\n",
    "                # Product sentence scores\n",
    "                sent2score = {}\n",
    "                for sentence in sentences:\n",
    "                    for word in nltk.word_tokenize(sentence.lower()):\n",
    "                        if word in word2count.keys():\n",
    "                            if len(sentence.split(' ')) < 25:\n",
    "                                if sentence not in sent2score.keys():\n",
    "                                    sent2score[sentence] = word2count[word]\n",
    "                                else:\n",
    "                                    sent2score[sentence] += word2count[word]\n",
    "\n",
    "                # Find the top sentence to use as a summary\n",
    "                if sent2score:\n",
    "                    summary_sentence = heapq.nlargest(1, sent2score, key=sent2score.get)[0]\n",
    "                    return summary_sentence\n",
    "                else:\n",
    "                    return clean_text  # Use the entire cleaned text as the summary\n",
    "\n",
    "            else:\n",
    "                return overview_text  # If not a string, return the original value\n",
    "\n",
    "        # Apply the summarization function to rows with null 'TagLine'\n",
    "        null_tagline_rows = X_modified[X_modified['TagLine'].isnull()]\n",
    "\n",
    "        # Apply the summarize_overview function to 'Overview'\n",
    "        summaries = X_modified.loc[null_tagline_rows.index, 'Overview'].apply(summarize_overview)\n",
    "\n",
    "        # Replace null values in 'TagLine' with the summarized values\n",
    "        X_modified.loc[null_tagline_rows.index, 'TagLine'] = summaries\n",
    "\n",
    "        # Removing OriginalTitle as it is same as Title but in original language\n",
    "        X_modified.drop(['OriginalTitle'], axis=1, inplace=True)\n",
    "\n",
    "        return X_modified\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "class RemoveColumnsTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_modified = X.copy(deep=True);\n",
    "        X_modified.drop(['Overview', 'Title', 'TagLine'], axis=1, inplace=True)\n",
    "        return X_modified\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "class ScaleTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.standardscalerBudget = MinMaxScaler()\n",
    "        self.standardscalerRunTime = MinMaxScaler()\n",
    "        self.standardscalerRevenue = MinMaxScaler()\n",
    "        self.onehotencoder = OneHotEncoder()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_modified = X.copy(deep=True)\n",
    "        \n",
    "        encoded = self.onehotencoder.fit_transform(X_modified[['Region']]).toarray()\n",
    "        encoded_df = pd.DataFrame(encoded, columns=self.onehotencoder.get_feature_names_out(['Region']), index=X_modified.index)\n",
    "        X_modified = pd.concat([X_modified, encoded_df], axis=1)\n",
    "        X_modified.drop(['Region'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        X_modified[['Budget']] = self.standardscalerBudget.fit_transform(X_modified[['Budget']])\n",
    "        X_modified[['RunTime']] = self.standardscalerRunTime.fit_transform(X_modified[['RunTime']])\n",
    "        X_modified[['Revenue']] = self.standardscalerRevenue.fit_transform(X_modified[['Revenue']])\n",
    "        return X_modified\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "column_transformer = ColumnTransformer(transformers=[\n",
    "    ('num', Pipeline(steps=[ ('scaler', MinMaxScaler()) ]), ['Budget', 'RunTime', 'Revenue']),\n",
    "    ('cat', Pipeline(steps=[ ('one-hot', OneHotEncoder()) ]), ['Region'])\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('genre_transformer', GenreTransformer()),\n",
    "    ('language_column', LanguageTransformer()),\n",
    "    ('country_transformer', CountryTransformer()),\n",
    "    ('OtherStepsTransformer', OtherStepsTransformer()),\n",
    "    ('ImputeBugetTransformer', ImputeBugetTransformer()),\n",
    "    ('PopularityTransformer', PopularityTransformer()),\n",
    "    ('TrimmedTransformer', TrimmedTransformer()),\n",
    "    ('SummarizeOverviewTransformer', SummarizeOverviewTransformer()),\n",
    "    ('RemoveColumnsTransformer', RemoveColumnsTransformer()),\n",
    "    ('column_transformer', ScaleTransformer())\n",
    "])\n",
    "\n",
    "df = pd.read_json('movies.json', orient='records')\n",
    "transformed_df = pipeline.fit_transform(df)\n",
    "\n",
    "y = transformed_df['Popularity_cat']\n",
    "X = transformed_df.drop('Popularity_cat', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, make_scorer, average_precision_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(decision_function_shape=&#x27;ovo&#x27;, probability=True, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(decision_function_shape=&#x27;ovo&#x27;, probability=True, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(decision_function_shape='ovo', probability=True, random_state=42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "svc_clf = SVC(probability=True, random_state=42, decision_function_shape='ovo')\n",
    "svc_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.49270664505672607\n",
      "precision 0.3499118752655963\n",
      "recall 0.49270664505672607\n",
      "f1 0.3651669425550771\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = svc_clf.predict(X_test)\n",
    "y_train_pred = svc_clf.predict(X_train)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print('accuracy', accuracy);\n",
    "print('precision', precision);\n",
    "print('recall', recall);\n",
    "print('f1', f1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GenreIds</th>\n",
       "      <th>Id</th>\n",
       "      <th>OriginalLanguage</th>\n",
       "      <th>OriginalTitle</th>\n",
       "      <th>Overview</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>ReleaseDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>VoteAverage</th>\n",
       "      <th>VoteCount</th>\n",
       "      <th>Budget</th>\n",
       "      <th>ProductionCompanies</th>\n",
       "      <th>ProductionCountries</th>\n",
       "      <th>SpokenLanguages</th>\n",
       "      <th>TagLine</th>\n",
       "      <th>RunTime</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[28, 12, 53]</td>\n",
       "      <td>299054</td>\n",
       "      <td>en</td>\n",
       "      <td>Expend4bles</td>\n",
       "      <td>Armed with every weapon they can get their han...</td>\n",
       "      <td>3741.062</td>\n",
       "      <td>2023-09-15</td>\n",
       "      <td>Expend4bles</td>\n",
       "      <td>6.4</td>\n",
       "      <td>364</td>\n",
       "      <td>100000000</td>\n",
       "      <td>[{'id': 1020, 'logo_path': '/kuUIHNwMec4dwOLgh...</td>\n",
       "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
       "      <td>[{'english_name': 'English', 'iso_639_1': 'en'...</td>\n",
       "      <td>They'll die when they're dead.</td>\n",
       "      <td>103</td>\n",
       "      <td>30000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       GenreIds      Id OriginalLanguage OriginalTitle  \\\n",
       "0  [28, 12, 53]  299054               en   Expend4bles   \n",
       "\n",
       "                                            Overview  Popularity ReleaseDate  \\\n",
       "0  Armed with every weapon they can get their han...    3741.062  2023-09-15   \n",
       "\n",
       "         Title  VoteAverage  VoteCount     Budget  \\\n",
       "0  Expend4bles          6.4        364  100000000   \n",
       "\n",
       "                                 ProductionCompanies  \\\n",
       "0  [{'id': 1020, 'logo_path': '/kuUIHNwMec4dwOLgh...   \n",
       "\n",
       "                                 ProductionCountries  \\\n",
       "0  [{'iso_3166_1': 'US', 'name': 'United States o...   \n",
       "\n",
       "                                     SpokenLanguages  \\\n",
       "0  [{'english_name': 'English', 'iso_639_1': 'en'...   \n",
       "\n",
       "                          TagLine  RunTime   Revenue  \n",
       "0  They'll die when they're dead.      103  30000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.read_json('movies.json', orient='records')\n",
    "# transformed_df = pipeline.fit_transform(df)\n",
    "\n",
    "# y = transformed_df['Popularity_cat']\n",
    "# X = transformed_df.drop('Popularity_cat', axis=1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "df_new.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 21\u001b[0m\n\u001b[0;32m      1\u001b[0m new_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenreIds\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[28, 12, 53]\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegion\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNorth America\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     20\u001b[0m })\n\u001b[1;32m---> 21\u001b[0m new_transformed \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m y_new_data \u001b[38;5;241m=\u001b[39m svc_clf\u001b[38;5;241m.\u001b[39mpredict(new_transformed\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevenue\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32md:\\Git\\advanced_python_project\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:696\u001b[0m, in \u001b[0;36mPipeline.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    694\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter():\n\u001b[1;32m--> 696\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Xt\n",
      "File \u001b[1;32md:\\Git\\advanced_python_project\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "Cell \u001b[1;32mIn[9], line 191\u001b[0m, in \u001b[0;36mCountryTransformer.transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    188\u001b[0m X_modified[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_modified[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProductionCountries\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: ast\u001b[38;5;241m.\u001b[39mliteral_eval(x)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mand\u001b[39;00m ast\u001b[38;5;241m.\u001b[39mliteral_eval(x) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    189\u001b[0m X_modified[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegion\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_modified[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcountry_region)\n\u001b[1;32m--> 191\u001b[0m X_modified[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[43mX_modified\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRegion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    193\u001b[0m X_modified\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProductionCountries\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    194\u001b[0m X_modified\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOverview\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Git\\advanced_python_project\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Git\\advanced_python_project\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1714\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1713\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m-> 1714\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32md:\\Git\\advanced_python_project\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1647\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1645\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[0;32m   1646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[1;32m-> 1647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "new_df = pd.DataFrame({\n",
    "    'GenreIds': ['[28, 12, 53]'],\n",
    "    'Id': [0],\n",
    "    'OriginalLanguage': ['en'],\n",
    "    'OriginalTitle': ['Expend4bles'], \n",
    "    'Overview': ['Armed with every weapon they can get their han'],\n",
    "    'Popularity': ['3741.062'], \n",
    "    'ReleaseDate': ['2023-09-15'], \n",
    "    'Title': ['Expend4bles'], \n",
    "    'VoteAverage': [6.4],\n",
    "    'VoteCount': [364],\n",
    "    'Budget': [100000000],\n",
    "    'ProductionCompanies': ['[]'],\n",
    "    'ProductionCountries': ['[]'],\n",
    "    'SpokenLanguages': ['[english]'],\n",
    "    'TagLine': [\"They'll die when they're dead.\"],\n",
    "    'RunTime': [103],\n",
    "    'Revenue': [30000000],\n",
    "    'Region': ['North America'],\n",
    "})\n",
    "new_transformed = pipeline.transform(new_df)\n",
    "y_new_data = svc_clf.predict(new_transformed.drop('Revenue', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "19HttZxEBXyu",
    "yMxmx6bsBXyy",
    "mj7rCgYwBXy0",
    "b3-nSwMlBXzK",
    "3idQF92SBXzM",
    "P9nfDr95BXzQ",
    "c_Jbz6b1BXy-",
    "VnL2ADxYBXzA",
    "q_fkX5-LBXzQ"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
